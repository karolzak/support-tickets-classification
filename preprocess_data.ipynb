{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T05:03:04.659545Z",
     "start_time": "2018-05-15T05:03:03.055089Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn import preprocessing\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from azureml.dataprep import package\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "removedWordsList = (['xxxxx1'])\n",
    "\n",
    "\n",
    "def removeNonEnglish(text, englishWords):\n",
    "    global removedWordsList\n",
    "    wordList = text.split()\n",
    "    if len(wordList) == 0:\n",
    "        return \" \"\n",
    "    y = np.array(wordList)\n",
    "    x = np.array(englishWords)\n",
    "    index = np.arange(len(englishWords))\n",
    "    sorted_index = np.searchsorted(x, y)\n",
    "    yindex = np.take(index, sorted_index, mode=\"clip\")\n",
    "    mask = x[yindex] != y\n",
    "    maskedArr = np.ma.array(yindex, mask=mask).compressed()\n",
    "    result = x[maskedArr]\n",
    "    text = np.array2string(result)\\\n",
    "        .replace(\"\\'\", \"\")\\\n",
    "        .replace(\"[\", \"\")\\\n",
    "        .replace(\"]\", \"\")\\\n",
    "        .replace(\"\\n\", \"\")\\\n",
    "        .replace(\"\\r\", \"\")\n",
    "\n",
    "    # Logging removed words\n",
    "    removedWords = set(wordList)-set(result)\n",
    "    removedWordsList += set(list(removedWords))-set(removedWordsList)\n",
    "    return text\n",
    "\n",
    "\n",
    "def encryptSingleColumn(data):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(data)\n",
    "    return le.transform(data)\n",
    "\n",
    "\n",
    "def encryptColumnsCollection(data, columnsToEncrypt):\n",
    "    for column in columnsToEncrypt:\n",
    "        data[column] = encryptSingleColumn(data[column])\n",
    "    return data\n",
    "\n",
    "\n",
    "def removeString(data, regex):\n",
    "    return data.str.lower().str.replace(regex.lower(), ' ')\n",
    "\n",
    "\n",
    "def cleanDataset(dataset, columnsToClean, regexList):\n",
    "    for column in columnsToClean:\n",
    "        for regex in regexList:\n",
    "            dataset[column] = removeString(dataset[column], regex)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def getRegexList():\n",
    "    regexList = []\n",
    "    regexList += ['From:(.*)\\r\\n']  # from line\n",
    "    # regexList += ['RITM[0-9]*'] # request id\n",
    "    # regexList += ['INC[0-9]*'] # incident id\n",
    "    # regexList += ['TKT[0-9]*'] # ticket id\n",
    "    regexList += ['Sent:(.*)\\r\\n']  # sent to line\n",
    "    regexList += ['Received:(.*)\\r\\n']  # received data line\n",
    "    regexList += ['To:(.*)\\r\\n']  # to line\n",
    "    regexList += ['CC:(.*)\\r\\n']  # cc line\n",
    "    regexList += ['The information(.*)infection']  # footer\n",
    "    regexList += ['Endava Limited is a company(.*)or omissions']  # footer\n",
    "    regexList += ['The information in this email is confidential and may be legally(.*)interference if you are not the intended recipient']  # footer\n",
    "    regexList += ['\\[cid:(.*)]']  # images cid\n",
    "    regexList += ['https?:[^\\]\\n\\r]+']  # https & http\n",
    "    regexList += ['Subject:']\n",
    "    # regexList += ['[\\w\\d\\-\\_\\.]+@[\\w\\d\\-\\_\\.]+']  # emails\n",
    "    # regexList += ['[0-9][\\-0â€“90-9 ]+']  # phones\n",
    "    # regexList += ['[0-9]']  # numbers\n",
    "    # regexList += ['[^a-zA-z 0-9]+']  # anything that is not a letter\n",
    "    # regexList += ['[\\r\\n]']  # \\r\\n\n",
    "    # regexList += [' [a-zA-Z] ']  # single letters\n",
    "    # regexList += [' [a-zA-Z][a-zA-Z] ']  # two-letter words\n",
    "    # regexList += [\"  \"]  # double spaces\n",
    "\n",
    "    regexList += ['^[_a-z0-9-]+(\\.[_a-z0-9-]+)*@[a-z0-9-]+(\\.[a-z0-9-]+)*(\\.[a-z]{2,4})$']\n",
    "    regexList += ['[\\w\\d\\-\\_\\.]+ @ [\\w\\d\\-\\_\\.]+']\n",
    "    regexList += ['Subject:']\n",
    "    regexList += ['[^a-zA-Z]']\n",
    "\n",
    "    return regexList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T05:03:30.130318Z",
     "start_time": "2018-05-15T05:03:04.665545Z"
    }
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# Use this with AML Workbench to load data from data prep file\n",
    "dfIncidents = package.run('Incidents.dprep', dataflow_idx=0)\n",
    "# dfIncidents = pd.read_csv('allIncidents.csv', encoding=\"ISO-8859-1\")\n",
    "dfRequests = package.run('Requests.dprep', dataflow_idx=0)\n",
    "# dfIncidents = package.run('IncidentsCleaned.dprep', dataflow_idx=0)\n",
    "# dfRequests = package.run('RequestsCleaned.dprep', dataflow_idx=0)\n",
    "\n",
    "# Load dataset from file\n",
    "# dfIncidents = pd.read_csv('./data/endava_tickets/all_incidents.csv')\n",
    "# dfRequests = pd.read_csv('./data/endava_tickets/all_requests.csv')\n",
    "#####################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T05:03:30.196341Z",
     "start_time": "2018-05-15T05:03:30.134320Z"
    }
   },
   "outputs": [],
   "source": [
    "print('dfIncidents shape:',dfIncidents.shape)\n",
    "print('dfRequests shape:',dfRequests.shape)\n",
    "dfIncidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T05:03:30.245336Z",
     "start_time": "2018-05-15T05:03:30.199373Z"
    }
   },
   "outputs": [],
   "source": [
    "dfRequests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T05:03:30.272049Z",
     "start_time": "2018-05-15T05:03:30.248336Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reorder columns\n",
    "columnsOrder = [\n",
    "    'title', 'body', 'ticket_type', 'category',\n",
    "    'sub_category1', 'sub_category2', 'business_service',\n",
    "    'urgency', 'impact'\n",
    "]\n",
    "dfIncidents = dfIncidents[columnsOrder]\n",
    "dfRequests = dfRequests[columnsOrder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T05:03:30.325051Z",
     "start_time": "2018-05-15T05:03:30.275016Z"
    }
   },
   "outputs": [],
   "source": [
    "dfIncidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T05:03:30.344054Z",
     "start_time": "2018-05-15T05:03:30.327021Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge incidents and requests datasets\n",
    "dfTickets = dfRequests.append(\n",
    "    dfIncidents,\n",
    "    ignore_index=True)  # set True to avoid index duplicates\n",
    "print(dfTickets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T05:03:30.476693Z",
     "start_time": "2018-05-15T05:03:30.347015Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "columnsToDropDuplicates = ['body']\n",
    "dfTickets = dfTickets.drop_duplicates(columnsToDropDuplicates)\n",
    "print(dfTickets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T05:03:30.485693Z",
     "start_time": "2018-05-15T05:03:30.480689Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge 'title' and 'body' columns into single column 'body'\n",
    "# dfTickets['body'] = (dfTickets['title']+\n",
    "#   \" \" + dfTickets['body']).map(str)\n",
    "# dfTickets = dfTickets.drop(['title'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove text with regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T05:03:50.737918Z",
     "start_time": "2018-05-15T05:03:30.487696Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select columns for cleaning\n",
    "columnsToClean = ['body', 'title']\n",
    "\n",
    "# Create list of regex to remove sensitive data\n",
    "# Clean dataset and remove sensitive data\n",
    "cleanDataset(dfTickets, columnsToClean, getRegexList())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Remove all non english words + names + blacklisted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T05:04:03.245842Z",
     "start_time": "2018-05-15T05:03:50.739917Z"
    }
   },
   "outputs": [],
   "source": [
    "# Firstly load english words dataset and names dataset\n",
    "# dfWordsEn = package.run('EnglishWords.dprep', dataflow_idx=0)\n",
    "# dfWordsEn = package.run('EnglishWordsAlpha.dprep', dataflow_idx=0)\n",
    "# dfWordsEn = package.run('EnglishWordsMerged.dprep', dataflow_idx=0)\n",
    "dfWordsEn = package.run('WordsEn.dprep', dataflow_idx=0)\n",
    "dfFirstNames = package.run('FirstNames.dprep', dataflow_idx=0)\n",
    "dfBlackListWords = package.run('WordsBlacklist.dprep', dataflow_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T05:04:03.324356Z",
     "start_time": "2018-05-15T05:04:03.253362Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transform all words to lower case\n",
    "dfWordsEn['Line'] = dfWordsEn['Line'].str.lower()\n",
    "dfFirstNames['Line'] = dfFirstNames['Line'].str.lower()\n",
    "dfBlackListWords['Line'] = dfBlackListWords['Line'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T05:04:03.450004Z",
     "start_time": "2018-05-15T05:04:03.326354Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge datasets removing names from English words dataset\n",
    "print(\"Shape before removing first names from\\\n",
    "    english words dataset: \"+str(dfWordsEn.shape))\n",
    "dfWords = dfWordsEn.merge(\n",
    "    dfFirstNames.drop_duplicates(),\n",
    "    on=['Line'], how='left', indicator=True)\n",
    "\n",
    "# Select words without names only\n",
    "dfWords = dfWords.loc[dfWords['_merge'] == 'left_only']\n",
    "print(\"Shape after removing first names from \\\n",
    "english words dataset: \"+str(dfWords.shape))\n",
    "dfWords = dfWords.drop(\"_merge\", axis=1)  # Drop merge indicator column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T05:04:03.572210Z",
     "start_time": "2018-05-15T05:04:03.453004Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge datasets removing blacklisted words\n",
    "print(\"Shape before removing blacklisted\\\n",
    "    words from english ords dataset: \"+str(dfWords.shape))\n",
    "dfWords = dfWords.merge(\n",
    "    dfBlackListWords.drop_duplicates(),\n",
    "    on=['Line'], how='left', indicator=True)\n",
    "\n",
    "# Select words\n",
    "dfWords = dfWords.loc[dfWords['_merge'] == 'left_only']\n",
    "print(\"Shape after removing blacklisted \\\n",
    "words from english words dataset: \"+str(dfWords.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T05:18:45.859543Z",
     "start_time": "2018-05-15T05:04:03.575174Z"
    }
   },
   "outputs": [],
   "source": [
    "print(dfTickets.shape)\n",
    "# Remove non english words and names\n",
    "dfTickets['body'] = dfTickets['body'].apply(\n",
    "    lambda emailBody: removeNonEnglish(emailBody, dfWords['Line']))\n",
    "print(dfTickets.shape)\n",
    "dfTickets['title'] = dfTickets['title'].apply(\n",
    "    lambda emailBody: removeNonEnglish(emailBody, dfWords['Line']))\n",
    "print(dfTickets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T05:18:45.989618Z",
     "start_time": "2018-05-15T05:18:45.862576Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove empty strings and null rows after removing non english words\n",
    "print(\"Before removing empty: \" + str(dfTickets.shape))\n",
    "dfTickets = dfTickets[dfTickets.body != \" \"]\n",
    "dfTickets = dfTickets[dfTickets.body != \"\"]\n",
    "dfTickets = dfTickets[~dfTickets.body.isnull()]\n",
    "print(\"After removing empty: \" + str(dfTickets.shape))\n",
    "\n",
    "# Remove duplicates x2\n",
    "columnsToDropDuplicates = ['body']\n",
    "dfTickets = dfTickets.drop_duplicates(columnsToDropDuplicates)\n",
    "print(\"After removing duplicates:\",dfTickets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data encryption and anonymization using LabelEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T05:18:47.070537Z",
     "start_time": "2018-05-15T05:18:45.992618Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select columns for encryption\n",
    "columnsToEncrypt = [\n",
    "    'category', 'sub_category1', 'sub_category2',\n",
    "    'business_service', 'urgency',\n",
    "    'impact', 'ticket_type'\n",
    "]\n",
    "\n",
    "# Encrypt data for each of selected columns\n",
    "dfTickets = encryptColumnsCollection(dfTickets, columnsToEncrypt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dataset and removed words to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned and encrypted dataset back to csv without indexes\n",
    "dfTickets.to_csv('all_tickets.csv', index=False, index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedRemovedWordsList = np.sort(removedWordsList)\n",
    "dfx = pd.DataFrame(sortedRemovedWordsList)\n",
    "dfx.to_csv(\"removed_words.csv\", index=False, index_label=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
